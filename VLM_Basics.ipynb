{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image depicts a busy urban scene, with a variety of activity occurring on a city\n",
      "street. Key elements include:\\n\\n- A crosswalk where several pedestrians are moving, including a person sitting on the\n",
      "ground and a boy engaged with a device.\\n- A few people sitting on a bench, with one reading a newspaper.\\n- A guitarist\n",
      "performing while a person on a bicycle passes by, and others riding scooters.\\n- Pigeons on the ground and a flower pot\n",
      "nearby, adding to the urban atmosphere.\\n- Vehicles are seen on the road, indicating traffic.\\n\\nThe overall scene seems\n",
      "to capture a moment of life in a bustling city.', role='assistant', function_call=None, tool_calls=None, refusal=None)\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'scene': 'A bustling city street with a mix of pedestrians and vehicles.',\n",
       "  'foreground': {'elements': [{'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'green jacket',\n",
       "      'activity': 'using a smartphone',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'lying down',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red hoodie',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red top',\n",
       "      'activity': 'reading a book',\n",
       "      'position': 'on a bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'older adult',\n",
       "      'clothing': 'suit',\n",
       "      'activity': 'reading a newspaper',\n",
       "      'position': 'on a bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'walking',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'casual outfit',\n",
       "      'activity': 'looking at a phone',\n",
       "      'position': 'on the sidewalk'}}],\n",
       "   'plants': {'type': 'flower pot',\n",
       "    'details': {'flowers': 'geraniums',\n",
       "     'position': 'next to the sitting boy'}},\n",
       "   'birds': {'type': 'pigeons',\n",
       "    'count': 'multiple',\n",
       "    'position': 'around the bench and ground'}},\n",
       "  'background': {'elements': [{'type': 'building',\n",
       "     'style': 'brick and glass',\n",
       "     'details': {'windows': 'large',\n",
       "      'stores': 'visible',\n",
       "      'color': 'brown and gray'}},\n",
       "    {'type': 'traffic light',\n",
       "     'state': 'yellow',\n",
       "     'position': 'above the street'},\n",
       "    {'type': 'vehicles',\n",
       "     'details': [{'type': 'car', 'color': 'gray', 'action': 'moving'},\n",
       "      {'type': 'motorcycle',\n",
       "       'action': 'riding',\n",
       "       'details': {'rider': 'wearing a helmet'}},\n",
       "      {'type': 'scooter', 'action': 'moving'}]},\n",
       "    {'type': 'street', 'details': {'crosswalk': 'visible', 'lines': 'white'}}],\n",
       "   'lighting': {'time': 'day', 'effect': 'soft sunlight'}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': [{'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'green jacket',\n",
       "    'activity': 'using a smartphone',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'lying down',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red hoodie',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red top',\n",
       "    'activity': 'reading a book',\n",
       "    'position': 'on a bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'older adult',\n",
       "    'clothing': 'suit',\n",
       "    'activity': 'reading a newspaper',\n",
       "    'position': 'on a bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'walking',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'casual outfit',\n",
       "    'activity': 'looking at a phone',\n",
       "    'position': 'on the sidewalk'}}],\n",
       " 'plants': {'type': 'flower pot',\n",
       "  'details': {'flowers': 'geraniums', 'position': 'next to the sitting boy'}},\n",
       " 'birds': {'type': 'pigeons',\n",
       "  'count': 'multiple',\n",
       "  'position': 'around the bench and ground'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numberOfPeople': 8,\n",
       " 'atmosphere': 'busy urban environment',\n",
       " 'hourOfTheDay': 17,\n",
       " 'people': [{'position': 'sitting',\n",
       "   'age': 12,\n",
       "   'activity': 'using a smartphone',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'lying down',\n",
       "   'age': 15,\n",
       "   'activity': 'unconscious',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'sitting',\n",
       "   'age': 60,\n",
       "   'activity': 'reading a newspaper',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'sitting',\n",
       "   'age': 30,\n",
       "   'activity': 'reading a book',\n",
       "   'gender': 'female'},\n",
       "  {'position': 'walking',\n",
       "   'age': 20,\n",
       "   'activity': 'using a smartphone',\n",
       "   'gender': 'female'},\n",
       "  {'position': 'riding a bicycle',\n",
       "   'age': 25,\n",
       "   'activity': 'cycling',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'walking',\n",
       "   'age': 35,\n",
       "   'activity': 'playing guitar',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'riding a scooter',\n",
       "   'age': 28,\n",
       "   'activity': 'scootering',\n",
       "   'gender': 'female'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give then a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the given scene, there is one individual in danger: a 15-year-old male who is lying down and unconscious. This situation requires immediate medical attention. \\n\\nThe Child Hospital should be alerted due to the age of the individual in danger.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, there are no specific coordinates given for the individuals in the image. However, since the 15-year-old male is described as \"lying down\" and \"unconscious,\" we can infer that he is likely in a location that is accessible to the busy urban environment, possibly near a sidewalk or public area where people are gathered.\\n\\nIf you need to provide coordinates for a first aid response, you would typically look for a nearby landmark or reference point in the busy urban environment to guide responders. For example, if the scene is near a park bench or a specific building, you could use that as a reference.\\n\\nIn a real-life scenario, it would be crucial to assess the situation quickly and ensure that emergency services are notified to attend to the unconscious individual. If you have more specific details about the layout or landmarks in the area, I could help you infer more precise coordinates.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 15-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[400, 600, 500, 700]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) doesn't work in a single, monolithic way.  Instead, it encompasses a broad range of techniques and approaches, all aimed at creating systems that can perform tasks that typically require human intelligence.  Here's a breakdown of some key concepts:\n",
      "\n",
      "**1. Data is King:**  AI systems, particularly machine learning (ML) models, rely heavily on data.  The more relevant and high-quality data you feed an AI, the better it will perform. This data is used to train the AI.\n",
      "\n",
      "**2. Algorithms are the Recipe:** Algorithms are sets of rules and calculations that the AI uses to process data.  Different algorithms are suited to different tasks. For example:\n",
      "\n",
      "* **Supervised Learning:** The algorithm learns from labeled data (data where the correct answer is already known).  Think of teaching a dog tricks – you show it what to do and reward it when it gets it right.  Examples include image classification (identifying cats vs. dogs) and spam detection.\n",
      "* **Unsupervised Learning:** The algorithm learns from unlabeled data, identifying patterns and structures on its own. This is like asking a child to sort toys into groups based on their similarities. Examples include clustering customers based on purchasing behavior and anomaly detection.\n",
      "* **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones.  Think of teaching a robot to walk – it tries different movements, gets rewarded for stable steps, and learns to avoid falling.  Examples include game playing (AlphaGo) and robotics.\n",
      "\n",
      "**3. Models are the Outcome:**  The training process using algorithms and data produces a *model*.  This model is essentially a mathematical representation of the patterns and relationships learned from the data.  It's what the AI uses to make predictions or decisions on new, unseen data.\n",
      "\n",
      "**4. Different Types of AI:**  Beyond the learning methods, AI is categorized in several ways:\n",
      "\n",
      "* **Narrow or Weak AI:**  This is the most common type of AI today.  It's designed to perform a specific task, like playing chess or recommending products.  It excels at its designated task but lacks general intelligence.\n",
      "* **General or Strong AI:**  This is hypothetical AI with human-level intelligence and the ability to learn and perform any intellectual task that a human being can.  We haven't achieved this yet.\n",
      "* **Artificial Superintelligence (ASI):** This is a hypothetical AI that surpasses human intelligence in all aspects.  This is also currently purely theoretical.\n",
      "\n",
      "**5. Deep Learning (a subset of ML):** Deep learning uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.  These networks are inspired by the structure and function of the human brain.  Deep learning has been particularly successful in areas like image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "**In Simple Terms:**  Imagine you want to teach a computer to identify cats in pictures.  You feed it thousands of cat pictures (data) labeled \"cat.\"  The algorithm (the learning method) analyzes these pictures, looking for common features (e.g., pointy ears, whiskers).  The resulting model can then identify cats in new pictures it's never seen before.\n",
      "\n",
      "It's important to note that AI is constantly evolving. New algorithms, techniques, and applications are being developed all the time, pushing the boundaries of what's possible.  The explanation above provides a basic framework, but the intricacies of different AI systems can be extremely complex.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[698,330,960,623]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
