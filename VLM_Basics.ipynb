{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"The image depicts a busy urban street scene. In the foreground, several people are\n",
      "interacting with their surroundings: a boy is sitting on the pavement, engrossed in his device, while a figure in a red\n",
      "hoodie appears to be lying on the ground. Nearby, a woman is seated on a bench reading, and a man in a suit is also\n",
      "seated, possibly reading or looking at something. \\n\\nIn the background, there are various vehicles and pedestrians. A\n",
      "musician with an acoustic guitar is walking across the street, and a cyclist is riding by. The environment features\n",
      "buildings, traffic lights, and some greenery, suggesting a lively city atmosphere. Pigeons are scattered around the\n",
      "area, adding to the urban scene's detail.\", role='assistant', function_call=None, tool_calls=None, refusal=None)\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'setting': 'A bustling urban street scene, likely in a city with tall buildings and a mix of modern and older architecture.',\n",
       "  'foreground': {'elements': [{'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'green jacket, shorts',\n",
       "      'activity': 'using a smartphone'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'lying down',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red hoodie',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red top, blue jeans',\n",
       "      'activity': 'reading a book'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'older adult',\n",
       "      'clothing': 'suit',\n",
       "      'activity': 'reading a newspaper'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'walking',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'pink top, shorts',\n",
       "      'activity': 'using a smartphone'}},\n",
       "    {'type': 'plant',\n",
       "     'details': {'type': 'flower pot', 'flowers': 'geraniums'}},\n",
       "    {'type': 'birds',\n",
       "     'details': {'species': 'pigeons',\n",
       "      'quantity': 'multiple',\n",
       "      'location': 'around the people'}}]},\n",
       "  'background': {'elements': [{'type': 'vehicles',\n",
       "     'details': {'types': ['car', 'taxi', 'scooter'], 'action': 'moving'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'biking',\n",
       "     'details': {'gender': 'male',\n",
       "      'clothing': 'black outfit',\n",
       "      'activity': 'riding a bicycle'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'walking',\n",
       "     'details': {'gender': 'male', 'clothing': 'black shirt, playing guitar'}},\n",
       "    {'type': 'traffic light', 'state': 'red'}]},\n",
       "  'atmosphere': {'lighting': 'warm, suggesting late afternoon or early evening',\n",
       "   'mood': 'busy, urban life'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': [{'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'green jacket, shorts',\n",
       "    'activity': 'using a smartphone'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'lying down',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red hoodie',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red top, blue jeans',\n",
       "    'activity': 'reading a book'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'older adult',\n",
       "    'clothing': 'suit',\n",
       "    'activity': 'reading a newspaper'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'walking',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'pink top, shorts',\n",
       "    'activity': 'using a smartphone'}},\n",
       "  {'type': 'plant', 'details': {'type': 'flower pot', 'flowers': 'geraniums'}},\n",
       "  {'type': 'birds',\n",
       "   'details': {'species': 'pigeons',\n",
       "    'quantity': 'multiple',\n",
       "    'location': 'around the people'}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this scene, the 16-year-old male who is lying down and unconscious is in danger. The normal hospital should be alerted for this situation.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, the 16-year-old male is described as \"lying down\" and \"unconscious.\" However, since there are no specific coordinates given in the data, I cannot provide exact coordinates.\\n\\nIn a real-life scenario, if you were to infer the coordinates based on the context of a busy and lively atmosphere at 5 PM, you might consider that the unconscious individual could be located in a more central area where people are likely to gather, such as near benches or open spaces where people are sitting or walking.\\n\\nIf you are in a situation where you need to assist this individual, it is crucial to approach them carefully, check for responsiveness, and call for emergency medical assistance immediately.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm unable to assist with that.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gava/.venv/openai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI works by mimicking human intelligence processes through algorithms and statistical models.  It doesn't \"think\" like a human, but rather processes information and makes predictions based on the data it's trained on.  Here's a breakdown of the key concepts:\n",
      "\n",
      "**1. Data is King:** AI systems are trained on massive datasets.  The quality and quantity of this data directly impacts the accuracy and effectiveness of the AI.  This data can be anything from images and text to sensor readings and financial transactions.\n",
      "\n",
      "**2. Algorithms are the Engine:** Algorithms are sets of rules and statistical techniques that the AI uses to analyze the data.  Different types of AI use different algorithms, depending on the task.  Common types include:\n",
      "\n",
      "* **Machine Learning (ML):**  This is a subset of AI where the system learns from data without explicit programming.  Instead of being explicitly programmed with rules, it identifies patterns and relationships in the data to make predictions or decisions.  There are several types of ML:\n",
      "    * **Supervised Learning:**  The algorithm is trained on labeled data (data with known outputs).  For example, showing an image classifier many pictures of cats and dogs, labeled accordingly, so it learns to distinguish between them.\n",
      "    * **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find structure or patterns on its own.  For example, clustering similar customers based on their purchasing behavior.\n",
      "    * **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones.  This is often used in robotics and game playing.\n",
      "\n",
      "* **Deep Learning (DL):** This is a more advanced form of ML that uses artificial neural networks with multiple layers (hence \"deep\").  These networks are inspired by the structure and function of the human brain, allowing them to process complex data and learn intricate patterns.  Deep learning is particularly effective for tasks like image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "* **Natural Language Processing (NLP):** This focuses on enabling computers to understand, interpret, and generate human language.  It's used in chatbots, machine translation, and sentiment analysis.\n",
      "\n",
      "* **Computer Vision:** This allows computers to \"see\" and interpret images and videos, enabling applications like facial recognition, object detection, and medical image analysis.\n",
      "\n",
      "\n",
      "**3. Training and Inference:** The AI system undergoes a training phase where it learns from the data using the chosen algorithm.  Once trained, it enters the inference phase, where it uses what it has learned to make predictions or decisions on new, unseen data.\n",
      "\n",
      "**4. Model Evaluation and Refinement:** After training, the AI model's performance is evaluated using various metrics.  Based on the evaluation, the model may be further refined through techniques like hyperparameter tuning or retraining with more data.\n",
      "\n",
      "\n",
      "**In simple terms:** Imagine teaching a dog a trick.  You show it (data), reward it when it does it right (training), and correct it when it's wrong (evaluation and refinement).  Eventually, the dog learns the trick (inference) and can perform it on command.  AI works similarly, but with algorithms and data instead of treats and corrections.\n",
      "\n",
      "It's important to note that AI is constantly evolving, and new techniques and algorithms are being developed all the time.  The field is incredibly broad, and this explanation provides a simplified overview of the core concepts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699, 331, 961, 624]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai)",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
