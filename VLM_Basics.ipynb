{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image depicts a busy urban street scene. It features various pedestrians and\n",
      "vehicles, with people engaged in different activities. \\n\\n- A young person is sitting on the ground, engrossed in a\n",
      "device.\\n- Nearby, a person appears to be lying on the pavement.\\n- Several birds are seen on the ground, likely\n",
      "pigeons.\\n- Other individuals are walking, cycling, or playing guitar in the background.\\n- Buildings and street\n",
      "furniture, such as a bench and a flower pot, complete the scene.\\n\\nThe atmosphere suggests a lively city environment\n",
      "with a mix of everyday activities.', role='assistant', function_call=None, tool_calls=None, refusal=None)\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'scene': 'A bustling city street with a mix of pedestrians and vehicles.',\n",
       "  'foreground': {'elements': [{'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'green jacket, shorts',\n",
       "      'activity': 'using a smartphone',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'lying down',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'red hoodie',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red top, blue jeans',\n",
       "      'activity': 'reading a book',\n",
       "      'position': 'on a bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'older',\n",
       "      'clothing': 'suit',\n",
       "      'activity': 'reading a newspaper',\n",
       "      'position': 'next to the young woman on the bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'walking',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'pink top, shorts',\n",
       "      'activity': 'looking at a phone',\n",
       "      'position': 'walking past the bench'}}],\n",
       "   'plants': {'type': 'flower pot',\n",
       "    'details': {'flowers': 'geraniums',\n",
       "     'position': 'next to the sitting boy'}},\n",
       "   'birds': {'type': 'pigeons',\n",
       "    'count': 'multiple',\n",
       "    'position': 'around the bench and ground'}},\n",
       "  'background': {'elements': [{'type': 'building',\n",
       "     'style': 'modern',\n",
       "     'details': {'windows': 'large, reflective',\n",
       "      'position': 'on the left side'}},\n",
       "    {'type': 'building',\n",
       "     'style': 'historic',\n",
       "     'details': {'features': 'steeple', 'position': 'in the background'}},\n",
       "    {'type': 'traffic',\n",
       "     'details': {'vehicles': [{'type': 'car',\n",
       "        'color': 'silver',\n",
       "        'position': 'in the street'},\n",
       "       {'type': 'motorcycle',\n",
       "        'action': 'passing',\n",
       "        'details': {'rider': 'wearing a helmet'}},\n",
       "       {'type': 'scooter',\n",
       "        'action': 'driving',\n",
       "        'details': {'rider': 'wearing a black jacket'}}],\n",
       "      'traffic_lights': {'color': 'yellow',\n",
       "       'position': 'above the street'}}}]},\n",
       "  'atmosphere': {'lighting': 'golden hour', 'mood': 'busy, urban life'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': [{'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'green jacket, shorts',\n",
       "    'activity': 'using a smartphone',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'lying down',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'red hoodie',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red top, blue jeans',\n",
       "    'activity': 'reading a book',\n",
       "    'position': 'on a bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'older',\n",
       "    'clothing': 'suit',\n",
       "    'activity': 'reading a newspaper',\n",
       "    'position': 'next to the young woman on the bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'walking',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'pink top, shorts',\n",
       "    'activity': 'looking at a phone',\n",
       "    'position': 'walking past the bench'}}],\n",
       " 'plants': {'type': 'flower pot',\n",
       "  'details': {'flowers': 'geraniums', 'position': 'next to the sitting boy'}},\n",
       " 'birds': {'type': 'pigeons',\n",
       "  'count': 'multiple',\n",
       "  'position': 'around the bench and ground'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numberOfPeople': 8,\n",
       " 'atmosphere': 'busy urban environment',\n",
       " 'hourOfTheDay': 17,\n",
       " 'people': [{'position': 'sitting',\n",
       "   'age': 12,\n",
       "   'activity': 'using a smartphone',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'lying down',\n",
       "   'age': 15,\n",
       "   'activity': 'unconscious',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'sitting',\n",
       "   'age': 30,\n",
       "   'activity': 'reading a newspaper',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'sitting',\n",
       "   'age': 25,\n",
       "   'activity': 'reading a book',\n",
       "   'gender': 'female'},\n",
       "  {'position': 'walking',\n",
       "   'age': 20,\n",
       "   'activity': 'walking with a phone',\n",
       "   'gender': 'female'},\n",
       "  {'position': 'riding a bicycle',\n",
       "   'age': 30,\n",
       "   'activity': 'cycling',\n",
       "   'gender': 'male'},\n",
       "  {'position': 'walking',\n",
       "   'age': 40,\n",
       "   'activity': 'walking',\n",
       "   'gender': 'female'},\n",
       "  {'position': 'riding a scooter',\n",
       "   'age': 35,\n",
       "   'activity': 'scootering',\n",
       "   'gender': 'male'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give then a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the given scene, there is one individual in danger: a 15-year-old male who is lying down and unconscious. This situation requires immediate attention. \\n\\nThe Child Hospital should be alerted due to the age of the individual in danger.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, there are no specific coordinates given for the individuals in the image. However, I can help you infer the likely coordinates for the 15-year-old male who is lying down and unconscious.\\n\\nIn a busy urban environment at 17:00, the 15-year-old might be located in a public area such as a park, sidewalk, or near a building. If we assume a typical layout, we can estimate the coordinates based on common spatial arrangements in such environments.\\n\\nFor example, if we consider a hypothetical coordinate system where:\\n- The x-axis represents the width of the area (0 to 100 meters)\\n- The y-axis represents the length of the area (0 to 100 meters)\\n\\nWe might infer the coordinates of the unconscious 15-year-old male to be around (50, 50), assuming he is in the center of a busy area where people are likely to gather.\\n\\nPlease note that this is purely an estimation and should not be used for any real-life application. In an actual emergency, it is crucial to assess the situation on-site and provide appropriate first aid.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 15-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[400, 600, 500, 700]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI works by mimicking human intelligence processes through algorithms and statistical models.  There's no single \"how it works\" because different AI approaches employ different techniques. However, some common underlying principles include:\n",
      "\n",
      "**1. Data Collection and Preparation:**  AI systems learn from data. This data needs to be collected, cleaned (removing errors and inconsistencies), and prepared (e.g., transformed into a format the algorithm understands).  The quality and quantity of data are crucial to the AI's performance.\n",
      "\n",
      "**2. Algorithm Selection:**  Different algorithms are suited to different tasks.  Key categories include:\n",
      "\n",
      "* **Machine Learning (ML):**  This is a broad category where systems learn from data without explicit programming.  ML algorithms identify patterns and relationships in data to make predictions or decisions.  Subcategories include:\n",
      "    * **Supervised Learning:** The algorithm learns from labeled data (data where the desired output is already known). Examples include image classification (identifying cats vs. dogs) and spam detection.\n",
      "    * **Unsupervised Learning:** The algorithm learns from unlabeled data, identifying patterns and structures without pre-defined categories.  Examples include clustering similar customers based on purchasing behavior and anomaly detection.\n",
      "    * **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment. It receives rewards for good actions and penalties for bad actions, learning to optimize its behavior over time.  Examples include game playing (e.g., AlphaGo) and robotics.\n",
      "\n",
      "* **Deep Learning (DL):** A subset of ML that uses artificial neural networks with multiple layers (hence \"deep\").  These networks are inspired by the structure and function of the human brain and are particularly effective at processing complex data like images, audio, and text.  Examples include image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "* **Expert Systems:**  These systems encode human knowledge and expertise into a set of rules. They use these rules to make decisions or solve problems in a specific domain.\n",
      "\n",
      "**3. Training the Model:**  The chosen algorithm is \"trained\" on the prepared data.  This involves feeding the data to the algorithm and adjusting its parameters to minimize errors and improve its performance. This process can be computationally intensive, especially for deep learning models.\n",
      "\n",
      "**4. Evaluation and Tuning:**  After training, the model's performance is evaluated using separate test data (data not used during training). This helps determine how well the model generalizes to new, unseen data.  Based on the evaluation, the model's parameters or the algorithm itself might be adjusted (tuned) to improve its accuracy and efficiency.\n",
      "\n",
      "**5. Deployment and Monitoring:** Once the model performs satisfactorily, it's deployed to perform its intended task (e.g., classifying images, translating text).  Its performance is continuously monitored to ensure it remains accurate and effective over time.  Models often need retraining as new data becomes available.\n",
      "\n",
      "\n",
      "In essence, AI works by combining large amounts of data, sophisticated algorithms, and significant computational power to create systems capable of learning, reasoning, and making decisions – mimicking aspects of human intelligence.  The specific techniques used depend heavily on the problem being solved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[693,327,964,625]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
