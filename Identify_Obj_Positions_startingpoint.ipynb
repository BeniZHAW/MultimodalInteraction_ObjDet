{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Object Positions in Images - YOLO vs VLM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n",
    "Taken from \n",
    "https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=wizbxA1lm-Tj\n",
    "https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Object_detection.ipynb#scrollTo=245bc92a470f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting Utils\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def plot_bounding_boxes(im, noun_phrases_and_positions):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each noun phrase, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        noun_phrases_and_positions: A list of tuples containing the noun phrases\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    'pink',\n",
    "    'purple',\n",
    "    'brown',\n",
    "    'gray',\n",
    "    'beige',\n",
    "    'turquoise',\n",
    "    'cyan',\n",
    "    'magenta',\n",
    "    'lime',\n",
    "    'navy',\n",
    "    'maroon',\n",
    "    'teal',\n",
    "    'olive',\n",
    "    'coral',\n",
    "    'lavender',\n",
    "    'violet',\n",
    "    'gold',\n",
    "    'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Iterate over the noun phrases and their positions\n",
    "    for i, (noun_phrase, (y1, x1, y2, x2)) in enumerate(\n",
    "        noun_phrases_and_positions):\n",
    "        # Select a color from the list\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        abs_x1 = int(x1/1000 * width)\n",
    "        abs_y1 = int(y1/1000 * height)\n",
    "        abs_x2 = int(x2/1000 * width)\n",
    "        abs_y2 = int(y2/1000 * height)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        draw.rectangle(\n",
    "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "        )\n",
    "\n",
    "        # Draw the text\n",
    "        draw.text((abs_x1 + 8, abs_y1 + 6), noun_phrase, fill=color)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "# @title Parsing utils\n",
    "def parse_list_boxes(text):\n",
    "  result = []\n",
    "  for line in text.strip().splitlines():\n",
    "    # Extract the numbers from the line, remove brackets and split by comma\n",
    "    try:\n",
    "      numbers = line.split('[')[1].split(']')[0].split(',')\n",
    "    except:\n",
    "      numbers =  line.split('- ')[1].split(',')\n",
    "\n",
    "    # Convert the numbers to integers and append to the result\n",
    "    result.append([int(num.strip()) for num in numbers])\n",
    "\n",
    "  return result\n",
    "\n",
    "def parse_list_boxes_with_label(text):\n",
    "  text = text.split(\"```\\n\")[0]\n",
    "  return json.loads(text.strip(\"```\").strip(\"python\").strip(\"json\").replace(\"'\", '\"').replace('\\n', '').replace(',}', '}'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.YOLO -> FUNKTIONIERT NICHT, LÃ–SUNGEN NEHMEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/table_scene.jpeg: 640x640 1 cup, 2 potted plants, 2 dining tables, 4 books, 3 vases, 296.3ms\n",
      "Speed: 25.4ms preprocess, 296.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([41., 58., 75., 75., 60., 73., 58., 73., 60., 73., 75., 73.])\n",
      "conf: tensor([0.8666, 0.7697, 0.7267, 0.7095, 0.6095, 0.6043, 0.5443, 0.4876, 0.3747, 0.3382, 0.2693, 0.2544])\n",
      "data: tensor([[6.1529e+02, 5.5336e+02, 8.4596e+02, 7.8049e+02, 8.6657e-01, 4.1000e+01],\n",
      "        [3.8491e+02, 8.2020e+01, 5.8930e+02, 3.5290e+02, 7.6967e-01, 5.8000e+01],\n",
      "        [7.2199e+02, 3.9503e+02, 9.6582e+02, 6.3202e+02, 7.2675e-01, 7.5000e+01],\n",
      "        [6.1139e+02, 3.5216e+02, 7.2161e+02, 5.2007e+02, 7.0952e-01, 7.5000e+01],\n",
      "        [0.0000e+00, 3.7149e+02, 1.0240e+03, 1.0240e+03, 6.0951e-01, 6.0000e+01],\n",
      "        [8.7753e+01, 4.1335e+02, 4.2453e+02, 5.5627e+02, 6.0427e-01, 7.3000e+01],\n",
      "        [5.8282e+02, 2.1022e+00, 1.0195e+03, 6.3871e+02, 5.4430e-01, 5.8000e+01],\n",
      "        [4.9829e-01, 6.3921e+02, 3.7148e+02, 8.9904e+02, 4.8757e-01, 7.3000e+01],\n",
      "        [0.0000e+00, 1.9840e+02, 3.8414e+02, 4.8186e+02, 3.7471e-01, 6.0000e+01],\n",
      "        [0.0000e+00, 6.1654e+02, 6.0315e+02, 8.9945e+02, 3.3820e-01, 7.3000e+01],\n",
      "        [3.9743e+02, 4.7641e+02, 5.5304e+02, 6.1034e+02, 2.6930e-01, 7.5000e+01],\n",
      "        [3.3171e+01, 5.0926e+02, 4.1046e+02, 6.2863e+02, 2.5437e-01, 7.3000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1024, 1024)\n",
      "shape: torch.Size([12, 6])\n",
      "xywh: tensor([[ 730.6226,  666.9278,  230.6720,  227.1285],\n",
      "        [ 487.1069,  217.4605,  204.3888,  270.8807],\n",
      "        [ 843.9022,  513.5225,  243.8283,  236.9861],\n",
      "        [ 666.4999,  436.1115,  110.2277,  167.9125],\n",
      "        [ 512.0000,  697.7327, 1024.0000,  652.4816],\n",
      "        [ 256.1396,  484.8136,  336.7734,  142.9195],\n",
      "        [ 801.1588,  320.4065,  436.6821,  636.6086],\n",
      "        [ 185.9875,  769.1257,  370.9783,  259.8381],\n",
      "        [ 192.0723,  340.1295,  384.1446,  283.4619],\n",
      "        [ 301.5774,  757.9977,  603.1547,  282.9135],\n",
      "        [ 475.2321,  543.3749,  155.6122,  133.9282],\n",
      "        [ 221.8134,  568.9494,  377.2842,  119.3703]])\n",
      "xywhn: tensor([[0.7135, 0.6513, 0.2253, 0.2218],\n",
      "        [0.4757, 0.2124, 0.1996, 0.2645],\n",
      "        [0.8241, 0.5015, 0.2381, 0.2314],\n",
      "        [0.6509, 0.4259, 0.1076, 0.1640],\n",
      "        [0.5000, 0.6814, 1.0000, 0.6372],\n",
      "        [0.2501, 0.4735, 0.3289, 0.1396],\n",
      "        [0.7824, 0.3129, 0.4264, 0.6217],\n",
      "        [0.1816, 0.7511, 0.3623, 0.2537],\n",
      "        [0.1876, 0.3322, 0.3751, 0.2768],\n",
      "        [0.2945, 0.7402, 0.5890, 0.2763],\n",
      "        [0.4641, 0.5306, 0.1520, 0.1308],\n",
      "        [0.2166, 0.5556, 0.3684, 0.1166]])\n",
      "xyxy: tensor([[6.1529e+02, 5.5336e+02, 8.4596e+02, 7.8049e+02],\n",
      "        [3.8491e+02, 8.2020e+01, 5.8930e+02, 3.5290e+02],\n",
      "        [7.2199e+02, 3.9503e+02, 9.6582e+02, 6.3202e+02],\n",
      "        [6.1139e+02, 3.5216e+02, 7.2161e+02, 5.2007e+02],\n",
      "        [0.0000e+00, 3.7149e+02, 1.0240e+03, 1.0240e+03],\n",
      "        [8.7753e+01, 4.1335e+02, 4.2453e+02, 5.5627e+02],\n",
      "        [5.8282e+02, 2.1022e+00, 1.0195e+03, 6.3871e+02],\n",
      "        [4.9829e-01, 6.3921e+02, 3.7148e+02, 8.9904e+02],\n",
      "        [0.0000e+00, 1.9840e+02, 3.8414e+02, 4.8186e+02],\n",
      "        [0.0000e+00, 6.1654e+02, 6.0315e+02, 8.9945e+02],\n",
      "        [3.9743e+02, 4.7641e+02, 5.5304e+02, 6.1034e+02],\n",
      "        [3.3171e+01, 5.0926e+02, 4.1046e+02, 6.2863e+02]])\n",
      "xyxyn: tensor([[6.0087e-01, 5.4039e-01, 8.2613e-01, 7.6220e-01],\n",
      "        [3.7589e-01, 8.0098e-02, 5.7549e-01, 3.4463e-01],\n",
      "        [7.0507e-01, 3.8577e-01, 9.4318e-01, 6.1720e-01],\n",
      "        [5.9706e-01, 3.4390e-01, 7.0470e-01, 5.0788e-01],\n",
      "        [0.0000e+00, 3.6278e-01, 1.0000e+00, 9.9997e-01],\n",
      "        [8.5696e-02, 4.0367e-01, 4.1458e-01, 5.4324e-01],\n",
      "        [5.6916e-01, 2.0529e-03, 9.9561e-01, 6.2374e-01],\n",
      "        [4.8661e-04, 6.2423e-01, 3.6277e-01, 8.7797e-01],\n",
      "        [0.0000e+00, 1.9375e-01, 3.7514e-01, 4.7057e-01],\n",
      "        [0.0000e+00, 6.0209e-01, 5.8902e-01, 8.7837e-01],\n",
      "        [3.8811e-01, 4.6524e-01, 5.4008e-01, 5.9603e-01],\n",
      "        [3.2394e-02, 4.9733e-01, 4.0084e-01, 6.1390e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "results = model('images/table_scene.jpeg', save = False)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/table_scene.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mplot_bounding_boxes\u001b[0;34m(im, noun_phrases_and_positions)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m im\n\u001b[0;32m---> 22\u001b[0m width, height \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create a drawing object\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "plot_bounding_boxes(\"images/table_scene.jpeg\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/table_scene.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfObjects\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of objects in the scene\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"objects\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of objects and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the object in the scene\"\n",
    "                            },\n",
    "                            \"colour\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Clour of object\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"size\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Size of the object, e.g., small, medium, big etc.\"\n",
    "                            },\n",
    "                            \"type\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"the type of object\"\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"colour\", \"size\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfObjects\", \"atmosphere\", \"hourOfTheDay\", \"objects\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numberOfObjects': 8,\n",
       " 'atmosphere': 'calm and cozy',\n",
       " 'hourOfTheDay': 10,\n",
       " 'objects': [{'type': 'book',\n",
       "   'colour': 'black',\n",
       "   'size': 'medium',\n",
       "   'position': 'top left'},\n",
       "  {'type': 'book',\n",
       "   'colour': 'black',\n",
       "   'size': 'medium',\n",
       "   'position': 'middle left'},\n",
       "  {'type': 'plant', 'colour': 'green', 'size': 'small', 'position': 'middle'},\n",
       "  {'type': 'plant', 'colour': 'green', 'size': 'medium', 'position': 'right'},\n",
       "  {'type': 'mug',\n",
       "   'colour': 'brown',\n",
       "   'size': 'medium',\n",
       "   'position': 'bottom right'},\n",
       "  {'type': 'glasses',\n",
       "   'colour': 'black',\n",
       "   'size': 'small',\n",
       "   'position': 'bottom left'},\n",
       "  {'type': 'phone',\n",
       "   'colour': 'black',\n",
       "   'size': 'small',\n",
       "   'position': 'bottom right'},\n",
       "  {'type': 'compass',\n",
       "   'colour': 'silver',\n",
       "   'size': 'small',\n",
       "   'position': 'bottom right'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)\n",
    "output_image_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[400, 550, 480, 600]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alert_sys_prompt = \"You are a special trained observer\"\n",
    "promptLLM(prompt =  \"Detect if there is a cup in the picture and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mplot_bounding_boxes\u001b[0;34m(im, noun_phrases_and_positions)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m im\n\u001b[0;32m---> 22\u001b[0m width, height \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create a drawing object\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "plot_bounding_boxes(img, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[543, 598, 755, 817]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a cup and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
